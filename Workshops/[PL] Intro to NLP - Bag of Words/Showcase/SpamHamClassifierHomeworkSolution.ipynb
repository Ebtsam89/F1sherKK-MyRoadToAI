{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem tego ćwiczenia jest stworzenie prostego modelu, który będzie w stanie rozróżniać normalne wiadomości (tzw. `ham`) od wiadomośći typu `spam`. Do przygodowania danych, zostanie wykorzystana technika z `Bag of Words`, która zostanie zaimplementowana bez użycia biblioteki `sklearn` w celu zrozumienia jej dokładniejszego działania.\n",
    "\n",
    "**W tym ćwiczeniu będzie dużo mniej szczegółowych opisów :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wartości stałe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane pochodzą z https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table(\"https://www.dropbox.com/s/0jb18jj8ve8xh2c/SMSSpamCollection?dl=1\",\n",
    "                   sep='\\t', header=None, names=['label', 'sms_message'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przegląd danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Wyświetl z iloma wierszami mamy do czynienia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilośc próbek: 5572\n"
     ]
    }
   ],
   "source": [
    "number_of_samples = df.shape[0]\n",
    "print(\"Ilośc próbek: {}\".format(number_of_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Wyświetl unikalne wartości kolumny `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasy, którch model będzie uczony: ['ham' 'spam']\n"
     ]
    }
   ],
   "source": [
    "unique_values = df[\"label\"].unique()\n",
    "print(\"Klasy, którch model będzie uczony: {}\".format(unique_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Wyświetl ilość próbek należacych odpowiednio do każdej klasy z osobna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Wyświetl 3 wiadomości typu `ham` oraz 3 wiadomości typu `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ham = df.loc[df[\"label\"] == \"ham\"]\n",
    "df_spam = df.loc[df[\"label\"] == \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol\n"
     ]
    }
   ],
   "source": [
    "print(df_ham[\"sms_message\"].iloc[17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its on in engalnd! But telly has decided it won't let me watch it and mia and elliot were kissing! Damn it!\n"
     ]
    }
   ],
   "source": [
    "print(df_ham[\"sms_message\"].iloc[1589])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hows the street where the end of library walk is?\n"
     ]
    }
   ],
   "source": [
    "print(df_ham[\"sms_message\"].iloc[3242])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Msg: get Gnarls Barkleys \"Crazy\" ringtone TOTALLY FREE just reply GO to this message right now!\n"
     ]
    }
   ],
   "source": [
    "print(df_spam[\"sms_message\"].iloc[523])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ur chance to win a £250 cash every wk TXT: ACTION to 80608. T's&C's www.movietrivia.tv custcare 08712405022, 1x150p/wk\n"
     ]
    }
   ],
   "source": [
    "print(df_spam[\"sms_message\"].iloc[234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get the official ENGLAND poly ringtone or colour flag on yer mobile for tonights game! Text TONE or FLAG to 84199. Optout txt ENG STOP Box39822 W111WX £1.50\n"
     ]
    }
   ],
   "source": [
    "print(df_spam[\"sms_message\"].iloc[485])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Zastanów się:\n",
    "- co jest wartością docelową (tym co model będzie musiał przewidywać)\n",
    "- która cecha będzie użyta do uczenia modelu\n",
    "- czy ilośc próbek przypadających na każdą klasę jest równa\n",
    "- jakiej metryki warto użyć do walidacji modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych\n",
    "\n",
    "### - Kolumna `label` powinna zawierać wartośći binarne 0 lub 1\n",
    "Użyj obiektu `LabelEncoder` biblioteki `sklearn` albo poeksperymentuj z funkcją `.map` obiektu `Dataframe`.\n",
    "- link do dokumentacji: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "- link do dokumentacji: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.label.map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Implementacja `Bag of Words` bez użycia `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając dany następujące wiadomości:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Hello!, How are you?\",\n",
    "        \"Work from home is great.\",\n",
    "        \"Do you like ice cream?\",\n",
    "        \"I love to sleep at home.\",\n",
    "        \"Hello, can I call you now?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Zamień wszystkie słowa w każdym ze stringów na `lowercase`\n",
    "Możesz tego dokonać poprzez wywołanie metody `.lower()` na obiekcie string. Następnie zapisz rezultat w liście `data_lowercase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lowercase = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lowercase.extend([s.lower() for s in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello!, how are you?', 'work from home is great.', 'do you like ice cream?', 'i love to sleep at home.', 'hello, can i call you now?']\n"
     ]
    }
   ],
   "source": [
    "print(data_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Usuń wszystkie znaki specjalnie\n",
    "Stwórz liste punktuacji, których chcesz się pozbyć. \n",
    "\n",
    "    punctuations = [\".\", \",\"]\n",
    "    \n",
    "Następnie przeiteruj po każdym znaku w każdym stringu i omiń te, które należą do tej listy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = [\".\", \",\", \"!\", \"?\", \"-\", \"_\", \"&\", \"$\", \"£\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_punctuations = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in data_lowercase:\n",
    "    string_cleaned = \"\"\n",
    "    \n",
    "    for character in string:\n",
    "        if character not in punctuations:\n",
    "            string_cleaned += character\n",
    "    \n",
    "    data_no_punctuations.append(string_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello how are you', 'work from home is great', 'do you like ice cream', 'i love to sleep at home', 'hello can i call you now']\n"
     ]
    }
   ],
   "source": [
    "print(data_no_punctuations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Obuduj napisany kod w funkcję `clean(string)`.\n",
    "Funkcja clean powinna zamieniać wszystkie słowa na `lowercase` i usuwać znaki specjalne ze stringa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    string_lowercase = string.lower()\n",
    "    \n",
    "    punctuations = [\".\", \",\", \"!\", \"?\", \"-\", \"_\", \"&\", \"$\", \"£\"]    \n",
    "    \n",
    "    string_cleaned = \"\"\n",
    "    for character in string_lowercase:\n",
    "        if character not in punctuations:\n",
    "            string_cleaned += character\n",
    "    \n",
    "    return string_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert clean(\"Hello!, How are you?\") == \"hello how are you\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Rozdziel wszystkie słowa i umieść je w liście\n",
    "Aby tego dokonać użyj metody `split()` na każdym stringu. Docelowo rozdzieli on słowa według znaku `\" \"` i zapisze w liście, którą następnie trzeba będzie dołączyc do listy `data_splited`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splited = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in data_no_punctuations:\n",
    "    words = string.split()\n",
    "    data_splited.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', 'you', 'work', 'from', 'home', 'is', 'great', 'do', 'you', 'like', 'ice', 'cream', 'i', 'love', 'to', 'sleep', 'at', 'home', 'hello', 'can', 'i', 'call', 'you', 'now']\n"
     ]
    }
   ],
   "source": [
    "print(data_splited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Zlicz częstość występowania każdego słowa w danych\n",
    "Jesteśmy w stanie tego dokonać dzięki obiektowi `Counter`:\n",
    "- link do dokumentacji: https://docs.python.org/2/library/collections.html\n",
    "\n",
    "Api to użycia tego obiektu jest następujące:\n",
    "    \n",
    "    counter = Counter(lista_danych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(data_splited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'you': 3, 'hello': 2, 'home': 2, 'i': 2, 'how': 1, 'are': 1, 'work': 1, 'from': 1, 'is': 1, 'great': 1, 'do': 1, 'like': 1, 'ice': 1, 'cream': 1, 'love': 1, 'to': 1, 'sleep': 1, 'at': 1, 'can': 1, 'call': 1, 'now': 1})\n"
     ]
    }
   ],
   "source": [
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie można odwołać się do częstości występowania każdego słowa jak do słownika:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter[\"home\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Stwórz słownik według wzorca `id`:`słowo`\n",
    "`id` powinny być przydzielone malejąco, według częstośći wsystępowania słowa (najczęsciej występujące słowo `id=0`, kolejne `id=1` itd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = dict()\n",
    "\n",
    "sorted_word_freq_list = sorted(list(counter.items()), key = lambda x: x[1], reverse=True) \n",
    "for i, (word, frequency) in enumerate(sorted_word_freq_list):\n",
    "    id_to_word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'you', 1: 'hello', 2: 'home', 3: 'i', 4: 'how', 5: 'are', 6: 'work', 7: 'from', 8: 'is', 9: 'great', 10: 'do', 11: 'like', 12: 'ice', 13: 'cream', 14: 'love', 15: 'to', 16: 'sleep', 17: 'at', 18: 'can', 19: 'call', 20: 'now'}\n"
     ]
    }
   ],
   "source": [
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Obuduj napisany kod w funkcję `create_dictionary(list_of_strings)`:\n",
    "Funkcja:\n",
    "    \n",
    "    def create_dictionary(list_of_strings):\n",
    "        id_to_word = dict()\n",
    "        \n",
    "        // miejsce na kod\n",
    "        \n",
    "        return id_to_word, word_to_id\n",
    "        \n",
    "Powinna przyjmować liste stringów i zwracać słownik `id_to_word` oraz `word_to_id`. Celem stworzenia `id_to_word` jest przypisanie odpowiednich indeksów do słów na bazie ich frekfencji. Słownik `word_to_id` to odwrotność `id_to_word` i jest wygodniejszy do korzystania w momencie tokenizacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(list_of_strings):\n",
    "    id_to_word = dict()\n",
    "    word_to_id = dict()\n",
    "\n",
    "    data_splited = []\n",
    "    for string in list_of_strings:\n",
    "        words = string.split()\n",
    "        data_splited.extend(words)\n",
    "        \n",
    "    counter = Counter(data_splited)\n",
    "    sorted_word_freq_list = sorted(list(counter.items()), key = lambda x: x[1], reverse=True) \n",
    "    for i, (word, frequency) in enumerate(sorted_word_freq_list):\n",
    "        id_to_word[i] = word    \n",
    "    \n",
    "    word_to_id = {w: i for (i, w) in id_to_word.items()}\n",
    "        \n",
    "    return id_to_word, word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Czyszczenie danych\n",
    "cleaned_data = [clean(s) for s in data]\n",
    "\n",
    "# Tworzenie słownika\n",
    "test_id_to_word, _ = create_dictionary(cleaned_data)\n",
    "\n",
    "assert test_id_to_word == id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Użyj słownika do zamienienia każdego zdania na wektor\n",
    "Ilość słów w słowniku jest równa wielkości wektora. Każde słowo ma przypisany do siebie indeks równoważny jego pozycji w wektorze. Każda pozycja w wektorze powina zawierać liczbę ile dane słowo, przypisane do tej pozycji, występuje w stringu.\n",
    "\n",
    "Przykładowo mając dany słownik `id_to_word`:\n",
    "\n",
    "    {\n",
    "        0: \"the\",\n",
    "        1: \"dog\",\n",
    "        2: \"over\",\n",
    "        3: \"jump\",\n",
    "        4: \"sheep\"\n",
    "    }\n",
    "  \n",
    "Zdanie:\n",
    "    \n",
    "    the dog jumped over the fox\n",
    "    \n",
    "Powinno zostać ztokenizowane jako:\n",
    "    \n",
    "    [2, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, 'dog': 1, 'over': 2, 'jump': 3, 'sheep': 4}\n"
     ]
    }
   ],
   "source": [
    "example_string = \"the dog jumped over the fox\"\n",
    "\n",
    "id_to_word_example = {\n",
    "    0: \"the\",\n",
    "    1: \"dog\",\n",
    "    2: \"over\",\n",
    "    3: \"jump\",\n",
    "    4: \"sheep\"\n",
    "}\n",
    "\n",
    "word_to_id_example = {w: i for i, w in id_to_word_example.items()}\n",
    "\n",
    "print(word_to_id_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: [0, 0, 0, 0, 0]\n",
      "After:  [2, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_string = [0] * len(example_dictionary)\n",
    "print(\"Before: {}\".format(tokenized_string))\n",
    "\n",
    "for word in example_string.split():\n",
    "    if word in word_to_id_example:\n",
    "        tokenized_string[word_to_id_example[word]] += 1\n",
    "        \n",
    "print(\"After:  {}\".format(tokenized_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenized_string == [2, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Obuduj kod w funkcję `tokenize(string, word_dict)`\n",
    "Funkcja powinna przyjmować pojedyńczy string i na bazie przesłanego słownika zamieniać go w odpowiedni wektor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string, word_dict):\n",
    "    tokenized_string = [0] * len(word_dict)\n",
    "    \n",
    "    for word in string.split():\n",
    "        if word in word_dict:\n",
    "            tokenized_string[word_dict[word]] += 1\n",
    "            \n",
    "    return tokenized_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize(example_string, word_to_id_example) == [2, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podział danych na zbiór testowy/treningowy\n",
    "- link: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimportuj funkcję `train_test_split` i użyj jej do podzielenia danych na zbiór treningowy/testowy w proporcji `0.8 (train) do 0.2 (test)`. Nie zapomnij ustawić parametru `random_state` na `RANDOM_SEED` aby odtworzyć wynik z notebooka zawierającego odpowiedź."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"sms_message\"], df[\"label\"], \n",
    "                                                    test_size=0.2, random_state=44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizacja\n",
    "Pora użyć funkcji, które zostały wcześniej:\n",
    "\n",
    "- wyczyść każdy string w zbiorach `X_train` oraz `X_test` przy użyciu funkcji `clean`,\n",
    "- zbuduj słownik na bazie zbioru `X_train` (`X_test` symuluje produkcje, więc mogą się tam znaleźć słowa, których model nigdy nie widział) przy użyciu funkcji `create_dictionary`,\n",
    "- dokonaj tokenizacji stringów ze zbiorów `X_train` oraz `X_test` przy pomocy funkcji `tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [clean(s) for s in X_train]\n",
    "X_test = [clean(s) for s in X_test]\n",
    "\n",
    "id_to_word, word_to_id = create_dictionary(X_train)\n",
    "\n",
    "X_train_tokenized = [tokenize(s, word_to_id) for s in X_train]\n",
    "X_test_tokenized = [tokenize(s, word_to_id) for s in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróć uwagę jak wygląda Twój słównik. Jeżeli występują tam znaki specjalnie \"przyklejone\" do zwykłych słów, to może warto wrócić się do funkcji `clean` i ją poprawić :)\n",
    "\n",
    "Może usunięcie cyfr to też dobry pomysł?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'i',\n",
       " 1: 'to',\n",
       " 2: 'you',\n",
       " 3: 'a',\n",
       " 4: 'the',\n",
       " 5: 'u',\n",
       " 6: 'and',\n",
       " 7: 'in',\n",
       " 8: 'is',\n",
       " 9: 'me',\n",
       " 10: 'my',\n",
       " 11: 'for',\n",
       " 12: 'your',\n",
       " 13: 'it',\n",
       " 14: 'of',\n",
       " 15: 'have',\n",
       " 16: 'call',\n",
       " 17: 'on',\n",
       " 18: 'that',\n",
       " 19: 'are',\n",
       " 20: '2',\n",
       " 21: 'now',\n",
       " 22: 'so',\n",
       " 23: 'but',\n",
       " 24: 'not',\n",
       " 25: 'or',\n",
       " 26: 'at',\n",
       " 27: 'do',\n",
       " 28: 'can',\n",
       " 29: 'will',\n",
       " 30: 'be',\n",
       " 31: 'ur',\n",
       " 32: 'if',\n",
       " 33: 'get',\n",
       " 34: \"i'm\",\n",
       " 35: 'with',\n",
       " 36: 'we',\n",
       " 37: 'just',\n",
       " 38: 'this',\n",
       " 39: 'up',\n",
       " 40: 'no',\n",
       " 41: 'when',\n",
       " 42: 'go',\n",
       " 43: 'lt;#gt;',\n",
       " 44: '4',\n",
       " 45: 'all',\n",
       " 46: 'from',\n",
       " 47: 'ok',\n",
       " 48: 'out',\n",
       " 49: 'what',\n",
       " 50: 'free',\n",
       " 51: 'like',\n",
       " 52: 'know',\n",
       " 53: 'how',\n",
       " 54: 'good',\n",
       " 55: 'am',\n",
       " 56: 'then',\n",
       " 57: 'got',\n",
       " 58: 'come',\n",
       " 59: 'was',\n",
       " 60: 'its',\n",
       " 61: 'only',\n",
       " 62: 'there',\n",
       " 63: 'time',\n",
       " 64: 'love',\n",
       " 65: 'he',\n",
       " 66: 'day',\n",
       " 67: 'want',\n",
       " 68: 'as',\n",
       " 69: 'send',\n",
       " 70: 'by',\n",
       " 71: \"i'll\",\n",
       " 72: 'going',\n",
       " 73: 'text',\n",
       " 74: 'ü',\n",
       " 75: 'home',\n",
       " 76: 'one',\n",
       " 77: 'lor',\n",
       " 78: 'need',\n",
       " 79: 'about',\n",
       " 80: 'r',\n",
       " 81: 'back',\n",
       " 82: 'sorry',\n",
       " 83: 'txt',\n",
       " 84: 'today',\n",
       " 85: 'still',\n",
       " 86: 'our',\n",
       " 87: 'see',\n",
       " 88: 'n',\n",
       " 89: 'stop',\n",
       " 90: \"don't\",\n",
       " 91: 'reply',\n",
       " 92: 'she',\n",
       " 93: 'later',\n",
       " 94: 'tell',\n",
       " 95: 'dont',\n",
       " 96: 'mobile',\n",
       " 97: 'new',\n",
       " 98: 'been',\n",
       " 99: 'take',\n",
       " 100: 'think',\n",
       " 101: 'any',\n",
       " 102: 'her',\n",
       " 103: 'da',\n",
       " 104: 'they',\n",
       " 105: 'please',\n",
       " 106: 'did',\n",
       " 107: 'night',\n",
       " 108: 'here',\n",
       " 109: 'phone',\n",
       " 110: 'hey',\n",
       " 111: 'some',\n",
       " 112: 'dear',\n",
       " 113: 'hi',\n",
       " 114: 'who',\n",
       " 115: 'has',\n",
       " 116: 'him',\n",
       " 117: 'an',\n",
       " 118: 'oh',\n",
       " 119: 'hope',\n",
       " 120: 'much',\n",
       " 121: 'had',\n",
       " 122: 'week',\n",
       " 123: 'more',\n",
       " 124: 'claim',\n",
       " 125: 'where',\n",
       " 126: 'happy',\n",
       " 127: 'too',\n",
       " 128: 'well',\n",
       " 129: 'way',\n",
       " 130: 'pls',\n",
       " 131: 'd',\n",
       " 132: \"it's\",\n",
       " 133: 'wat',\n",
       " 134: 'should',\n",
       " 135: 'great',\n",
       " 136: 'give',\n",
       " 137: 'make',\n",
       " 138: 'say',\n",
       " 139: 'work',\n",
       " 140: 'right',\n",
       " 141: 'doing',\n",
       " 142: 'ask',\n",
       " 143: 'amp;',\n",
       " 144: 'number',\n",
       " 145: 'already',\n",
       " 146: 'tomorrow',\n",
       " 147: 'prize',\n",
       " 148: 'im',\n",
       " 149: 'after',\n",
       " 150: '1',\n",
       " 151: 'them',\n",
       " 152: 'would',\n",
       " 153: 'really',\n",
       " 154: 'yeah',\n",
       " 155: 'anything',\n",
       " 156: 'e',\n",
       " 157: 'c',\n",
       " 158: ':)',\n",
       " 159: 'yes',\n",
       " 160: 'message',\n",
       " 161: 'every',\n",
       " 162: 'last',\n",
       " 163: 'miss',\n",
       " 164: 'why',\n",
       " 165: 'said',\n",
       " 166: 'meet',\n",
       " 167: 'msg',\n",
       " 168: 'lol',\n",
       " 169: 'very',\n",
       " 170: 'life',\n",
       " 171: 'let',\n",
       " 172: 'b',\n",
       " 173: 'babe',\n",
       " 174: 'sure',\n",
       " 175: 'find',\n",
       " 176: 'morning',\n",
       " 177: 'something',\n",
       " 178: 'care',\n",
       " 179: 'cos',\n",
       " 180: '3',\n",
       " 181: 'also',\n",
       " 182: 'pick',\n",
       " 183: 'k',\n",
       " 184: 'nokia',\n",
       " 185: 'over',\n",
       " 186: 'cash',\n",
       " 187: 'buy',\n",
       " 188: 'win',\n",
       " 189: 'gud',\n",
       " 190: 'thanks',\n",
       " 191: 'which',\n",
       " 192: 'keep',\n",
       " 193: 'even',\n",
       " 194: 'again',\n",
       " 195: 'ill',\n",
       " 196: 'sent',\n",
       " 197: 'contact',\n",
       " 198: 'down',\n",
       " 199: 'around',\n",
       " 200: 'went',\n",
       " 201: 'won',\n",
       " 202: 'before',\n",
       " 203: 'next',\n",
       " 204: 'us',\n",
       " 205: 'wait',\n",
       " 206: 'cant',\n",
       " 207: 'thing',\n",
       " 208: 'first',\n",
       " 209: 'urgent',\n",
       " 210: 'try',\n",
       " 211: \"can't\",\n",
       " 212: 'could',\n",
       " 213: \"that's\",\n",
       " 214: 'off',\n",
       " 215: 'tonight',\n",
       " 216: 'feel',\n",
       " 217: 'late',\n",
       " 218: \"i've\",\n",
       " 219: 'someone',\n",
       " 220: 'customer',\n",
       " 221: 'always',\n",
       " 222: 'gonna',\n",
       " 223: 'things',\n",
       " 224: 'friends',\n",
       " 225: 'waiting',\n",
       " 226: 'nice',\n",
       " 227: 'were',\n",
       " 228: 'place',\n",
       " 229: 'his',\n",
       " 230: 'money',\n",
       " 231: 'chat',\n",
       " 232: 'wan',\n",
       " 233: 'told',\n",
       " 234: 'other',\n",
       " 235: 'service',\n",
       " 236: 'done',\n",
       " 237: 'year',\n",
       " 238: 'per',\n",
       " 239: \"you're\",\n",
       " 240: 'thk',\n",
       " 241: 'same',\n",
       " 242: 'soon',\n",
       " 243: 'x',\n",
       " 244: 'help',\n",
       " 245: 'sleep',\n",
       " 246: 'getting',\n",
       " 247: 'leave',\n",
       " 248: 'many',\n",
       " 249: 'tone',\n",
       " 250: 'ya',\n",
       " 251: 'guaranteed',\n",
       " 252: 'lunch',\n",
       " 253: 'cool',\n",
       " 254: 'long',\n",
       " 255: 'dun',\n",
       " 256: 'person',\n",
       " 257: 'wish',\n",
       " 258: 'may',\n",
       " 259: 'best',\n",
       " 260: 'holiday',\n",
       " 261: 'class',\n",
       " 262: 'yup',\n",
       " 263: 'few',\n",
       " 264: 'hello',\n",
       " 265: 'draw',\n",
       " 266: 'people',\n",
       " 267: 'friend',\n",
       " 268: 'mins',\n",
       " 269: 'use',\n",
       " 270: 'yet',\n",
       " 271: 'bit',\n",
       " 272: 'finish',\n",
       " 273: 'better',\n",
       " 274: '500',\n",
       " 275: 'end',\n",
       " 276: \"didn't\",\n",
       " 277: 'stuff',\n",
       " 278: 'being',\n",
       " 279: 'never',\n",
       " 280: '5',\n",
       " 281: 'having',\n",
       " 282: 'haha',\n",
       " 283: 'smile',\n",
       " 284: 'than',\n",
       " 285: 'man',\n",
       " 286: 'fine',\n",
       " 287: '100',\n",
       " 288: 'trying',\n",
       " 289: 'heart',\n",
       " 290: 'special',\n",
       " 291: 'coming',\n",
       " 292: 'live',\n",
       " 293: 'y',\n",
       " 294: 'mind',\n",
       " 295: 'because',\n",
       " 296: 'name',\n",
       " 297: 'sms',\n",
       " 298: 'thats',\n",
       " 299: 'line',\n",
       " 300: 'car',\n",
       " 301: 'talk',\n",
       " 302: 'guys',\n",
       " 303: 'yo',\n",
       " 304: 'job',\n",
       " 305: '*',\n",
       " 306: 'half',\n",
       " 307: 'wont',\n",
       " 308: '1st',\n",
       " 309: 'account',\n",
       " 310: 'v',\n",
       " 311: 'chance',\n",
       " 312: 'awarded',\n",
       " 313: 'shows',\n",
       " 314: 'nothing',\n",
       " 315: 'luv',\n",
       " 316: 'guess',\n",
       " 317: 'into',\n",
       " 318: 'check',\n",
       " 319: 'minutes',\n",
       " 320: 'dinner',\n",
       " 321: 'dat',\n",
       " 322: 'ready',\n",
       " 323: 'latest',\n",
       " 324: 'eat',\n",
       " 325: 'meeting',\n",
       " 326: 'days',\n",
       " 327: 'landline',\n",
       " 328: 'thought',\n",
       " 329: 'house',\n",
       " 330: 'bad',\n",
       " 331: 'camera',\n",
       " 332: 'box',\n",
       " 333: 'lot',\n",
       " 334: 'big',\n",
       " 335: 'lar',\n",
       " 336: '150',\n",
       " 337: 'might',\n",
       " 338: 'face',\n",
       " 339: 'jus',\n",
       " 340: '6',\n",
       " 341: 'enjoy',\n",
       " 342: 'left',\n",
       " 343: 'early',\n",
       " 344: 'another',\n",
       " 345: 'shit',\n",
       " 346: 'birthday',\n",
       " 347: 'start',\n",
       " 348: '1000',\n",
       " 349: 'sir',\n",
       " 350: 'room',\n",
       " 351: 'didnt',\n",
       " 352: '150ppm',\n",
       " 353: 'pay',\n",
       " 354: 'den',\n",
       " 355: 'quite',\n",
       " 356: 'aight',\n",
       " 357: 'liao',\n",
       " 358: 'god',\n",
       " 359: 'thanx',\n",
       " 360: 'made',\n",
       " 361: 'pa',\n",
       " 362: 'real',\n",
       " 363: 'ever',\n",
       " 364: 'called',\n",
       " 365: 'little',\n",
       " 366: 'does',\n",
       " 367: 'leh',\n",
       " 368: 'sweet',\n",
       " 369: 'min',\n",
       " 370: 'probably',\n",
       " 371: 'bed',\n",
       " 372: 'play',\n",
       " 373: 'watching',\n",
       " 374: 'wanna',\n",
       " 375: 'hear',\n",
       " 376: 'afternoon',\n",
       " 377: 'maybe',\n",
       " 378: 'month',\n",
       " 379: 'speak',\n",
       " 380: 'receive',\n",
       " 381: 'thank',\n",
       " 382: \"he's\",\n",
       " 383: 'though',\n",
       " 384: 'network',\n",
       " 385: 'cost',\n",
       " 386: 'anyway',\n",
       " 387: 'reach',\n",
       " 388: 'hour',\n",
       " 389: 'bus',\n",
       " 390: 'xxx',\n",
       " 391: 'princess',\n",
       " 392: 'actually',\n",
       " 393: 'weekend',\n",
       " 394: 'girl',\n",
       " 395: 'problem',\n",
       " 396: 'boy',\n",
       " 397: 'hav',\n",
       " 398: 'fuck',\n",
       " 399: 'po',\n",
       " 400: 'okay',\n",
       " 401: 'dunno',\n",
       " 402: 'working',\n",
       " 403: 'tmr',\n",
       " 404: 'plan',\n",
       " 405: 'everything',\n",
       " 406: 'baby',\n",
       " 407: 'sat',\n",
       " 408: 'abt',\n",
       " 409: 'watch',\n",
       " 410: 'forgot',\n",
       " 411: 'join',\n",
       " 412: 'town',\n",
       " 413: 'tones',\n",
       " 414: 's',\n",
       " 415: 'dis',\n",
       " 416: 'apply',\n",
       " 417: 'sexy',\n",
       " 418: 'important',\n",
       " 419: 'once',\n",
       " 420: '2nd',\n",
       " 421: 'says',\n",
       " 422: 'fun',\n",
       " 423: '2000',\n",
       " 424: 'ah',\n",
       " 425: 'remember',\n",
       " 426: 'texts',\n",
       " 427: 'until',\n",
       " 428: '16+',\n",
       " 429: 'shall',\n",
       " 430: 'part',\n",
       " 431: 'school',\n",
       " 432: 'enough',\n",
       " 433: 'guy',\n",
       " 434: 'world',\n",
       " 435: 'easy',\n",
       " 436: 'bored',\n",
       " 437: 'look',\n",
       " 438: 'collect',\n",
       " 439: 'since',\n",
       " 440: 'two',\n",
       " 441: ':(',\n",
       " 442: 'without',\n",
       " 443: 'bt',\n",
       " 444: 'between',\n",
       " 445: 'wif',\n",
       " 446: 'shopping',\n",
       " 447: \"how's\",\n",
       " 448: 'put',\n",
       " 449: 'means',\n",
       " 450: 'gift',\n",
       " 451: 'weekly',\n",
       " 452: 'selected',\n",
       " 453: '+',\n",
       " 454: 'able',\n",
       " 455: 'video',\n",
       " 456: 'dude',\n",
       " 457: 'bring',\n",
       " 458: 'came',\n",
       " 459: 'wanted',\n",
       " 460: '7',\n",
       " 461: 'collection',\n",
       " 462: 'asked',\n",
       " 463: 'while',\n",
       " 464: 'offer',\n",
       " 465: 'wake',\n",
       " 466: '4*',\n",
       " 467: '16',\n",
       " 468: 'online',\n",
       " 469: \"there's\",\n",
       " 470: 'wen',\n",
       " 471: 'music',\n",
       " 472: 'goes',\n",
       " 473: 'award',\n",
       " 474: 'show',\n",
       " 475: 'evening',\n",
       " 476: 'together',\n",
       " 477: 'office',\n",
       " 478: 'alright',\n",
       " 479: 'mail',\n",
       " 480: 'xmas',\n",
       " 481: 'orange',\n",
       " 482: 'til',\n",
       " 483: 'tcs',\n",
       " 484: 'lei',\n",
       " 485: 'away',\n",
       " 486: 'plus',\n",
       " 487: 'else',\n",
       " 488: 'calls',\n",
       " 489: 'attempt',\n",
       " 490: 'hours',\n",
       " 491: 'happen',\n",
       " 492: 'dad',\n",
       " 493: 'makes',\n",
       " 494: 'wk',\n",
       " 495: 'run',\n",
       " 496: 'full',\n",
       " 497: 'entry',\n",
       " 498: 'true',\n",
       " 499: 'pain',\n",
       " 500: '800',\n",
       " 501: 'sch',\n",
       " 502: 'colour',\n",
       " 503: 'delivery',\n",
       " 504: 'price',\n",
       " 505: 'those',\n",
       " 506: 'beautiful',\n",
       " 507: 'family',\n",
       " 508: 'most',\n",
       " 509: 'wife',\n",
       " 510: 'hair',\n",
       " 511: 'missed',\n",
       " 512: 'valid',\n",
       " 513: 'hot',\n",
       " 514: 'oso',\n",
       " 515: 'tv',\n",
       " 516: 'must',\n",
       " 517: 'g',\n",
       " 518: 'plz',\n",
       " 519: 'wants',\n",
       " 520: 'stay',\n",
       " 521: 'yesterday',\n",
       " 522: 'mob',\n",
       " 523: 'yourself',\n",
       " 524: 'times',\n",
       " 525: 'vouchers',\n",
       " 526: 'words',\n",
       " 527: 'driving',\n",
       " 528: 'wil',\n",
       " 529: 'tc',\n",
       " 530: 'looking',\n",
       " 531: 'yours',\n",
       " 532: 'wot',\n",
       " 533: 'haf',\n",
       " 534: 'worry',\n",
       " 535: '9',\n",
       " 536: 'these',\n",
       " 537: 'nite',\n",
       " 538: 'ringtone',\n",
       " 539: 'hurt',\n",
       " 540: '5000',\n",
       " 541: 'sae',\n",
       " 542: 'started',\n",
       " 543: 'calling',\n",
       " 544: 'neva',\n",
       " 545: 'test',\n",
       " 546: 'trip',\n",
       " 547: 'comes',\n",
       " 548: '8007',\n",
       " 549: 'email',\n",
       " 550: 'address',\n",
       " 551: 'details',\n",
       " 552: 'cause',\n",
       " 553: \"won't\",\n",
       " 554: 'havent',\n",
       " 555: 'todays',\n",
       " 556: 'took',\n",
       " 557: 'update',\n",
       " 558: '250',\n",
       " 559: 'word',\n",
       " 560: '18',\n",
       " 561: 'coz',\n",
       " 562: 'tried',\n",
       " 563: 'tot',\n",
       " 564: 'old',\n",
       " 565: 'juz',\n",
       " 566: 'feeling',\n",
       " 567: 'gr8',\n",
       " 568: 'making',\n",
       " 569: 'food',\n",
       " 570: 'bonus',\n",
       " 571: 'ard',\n",
       " 572: \"what's\",\n",
       " 573: 'top',\n",
       " 574: 'saw',\n",
       " 575: 'years',\n",
       " 576: 'await',\n",
       " 577: 'saying',\n",
       " 578: 'question',\n",
       " 579: 'answer',\n",
       " 580: 'sun',\n",
       " 581: 'huh',\n",
       " 582: \"haven't\",\n",
       " 583: 'till',\n",
       " 584: 'busy',\n",
       " 585: 'change',\n",
       " 586: 'book',\n",
       " 587: 'land',\n",
       " 588: 'post',\n",
       " 589: 'minute',\n",
       " 590: 'leaving',\n",
       " 591: 'ring',\n",
       " 592: 'messages',\n",
       " 593: 'points',\n",
       " 594: 'date',\n",
       " 595: 'wid',\n",
       " 596: 'game',\n",
       " 597: 'de',\n",
       " 598: 'sad',\n",
       " 599: 'brother',\n",
       " 600: 'okie',\n",
       " 601: 'o',\n",
       " 602: 'double',\n",
       " 603: 'noe',\n",
       " 604: 'lt;decimalgt;',\n",
       " 605: 'movie',\n",
       " 606: '750',\n",
       " 607: 'anyone',\n",
       " 608: 'both',\n",
       " 609: 'lucky',\n",
       " 610: 'worth',\n",
       " 611: \"we'll\",\n",
       " 612: 'eve',\n",
       " 613: 'drop',\n",
       " 614: 'sister',\n",
       " 615: 'tomo',\n",
       " 616: 'second',\n",
       " 617: 'either',\n",
       " 618: 'kind',\n",
       " 619: 're',\n",
       " 620: 'order',\n",
       " 621: \"she's\",\n",
       " 622: 'head',\n",
       " 623: 'mean',\n",
       " 624: 'mobileupd8',\n",
       " 625: 'awesome',\n",
       " 626: 'private',\n",
       " 627: '10',\n",
       " 628: 'believe',\n",
       " 629: 'wonderful',\n",
       " 630: 'choose',\n",
       " 631: '150p',\n",
       " 632: 'light',\n",
       " 633: 'smth',\n",
       " 634: 'charge',\n",
       " 635: 'simple',\n",
       " 636: 'unsubscribe',\n",
       " 637: 'final',\n",
       " 638: 'pub',\n",
       " 639: \"we're\",\n",
       " 640: 'forget',\n",
       " 641: 'aft',\n",
       " 642: \"''\",\n",
       " 643: 'smiling',\n",
       " 644: 'lesson',\n",
       " 645: 'lose',\n",
       " 646: 'id',\n",
       " 647: 'decided',\n",
       " 648: 'sis',\n",
       " 649: 'mine',\n",
       " 650: 'angry',\n",
       " 651: 'wit',\n",
       " 652: 'break',\n",
       " 653: 'read',\n",
       " 654: 'open',\n",
       " 655: 'hard',\n",
       " 656: 'voucher',\n",
       " 657: 'boytoy',\n",
       " 658: 'statement',\n",
       " 659: 'm',\n",
       " 660: 'expires',\n",
       " 661: '86688',\n",
       " 662: 'each',\n",
       " 663: 'story',\n",
       " 664: 'college',\n",
       " 665: 'thinking',\n",
       " 666: '12hrs',\n",
       " 667: '08000839402',\n",
       " 668: 'search',\n",
       " 669: 'carlos',\n",
       " 670: 'smoke',\n",
       " 671: 'goin',\n",
       " 672: 'lots',\n",
       " 673: 't',\n",
       " 674: 'winner',\n",
       " 675: 'finished',\n",
       " 676: 'mate',\n",
       " 677: 'bout',\n",
       " 678: 'pobox',\n",
       " 679: 'rite',\n",
       " 680: '87066',\n",
       " 681: 'sounds',\n",
       " 682: 'hows',\n",
       " 683: 'set',\n",
       " 684: 'pretty',\n",
       " 685: 'nt',\n",
       " 686: 'gone',\n",
       " 687: 'walk',\n",
       " 688: 'used',\n",
       " 689: 'no:',\n",
       " 690: 'unredeemed',\n",
       " 691: 'identifier',\n",
       " 692: \"you've\",\n",
       " 693: 'club',\n",
       " 694: 'rate',\n",
       " 695: 'lets',\n",
       " 696: 'mu',\n",
       " 697: 'everyone',\n",
       " 698: 'nope',\n",
       " 699: '8',\n",
       " 700: 'taking',\n",
       " 701: 'dreams',\n",
       " 702: '@',\n",
       " 703: 'pounds',\n",
       " 704: 'drive',\n",
       " 705: \"doesn't\",\n",
       " 706: 'hold',\n",
       " 707: 'tho',\n",
       " 708: 'close',\n",
       " 709: 'shop',\n",
       " 710: 'poly',\n",
       " 711: 'whole',\n",
       " 712: 'knw',\n",
       " 713: 'listen',\n",
       " 714: 'frnd',\n",
       " 715: 'services',\n",
       " 716: 'earlier',\n",
       " 717: 'congrats',\n",
       " 718: 'drink',\n",
       " 719: '||',\n",
       " 720: 'whatever',\n",
       " 721: 'card',\n",
       " 722: 'happened',\n",
       " 723: 'askd',\n",
       " 724: 'th',\n",
       " 725: 'treat',\n",
       " 726: 'alone',\n",
       " 727: 'credit',\n",
       " 728: 'b4',\n",
       " 729: 'within',\n",
       " 730: 'content',\n",
       " 731: 'far',\n",
       " 732: 'friday',\n",
       " 733: 'john',\n",
       " 734: 'side',\n",
       " 735: 'frnds',\n",
       " 736: 'enter',\n",
       " 737: 'outside',\n",
       " 738: 'computer',\n",
       " 739: 'txts',\n",
       " 740: 'fr',\n",
       " 741: 'code:',\n",
       " 742: 'needs',\n",
       " 743: 'secret',\n",
       " 744: 'welcome',\n",
       " 745: 'felt',\n",
       " 746: 'meant',\n",
       " 747: 'parents',\n",
       " 748: 'wkly',\n",
       " 749: 'gd',\n",
       " 750: 'gas',\n",
       " 751: 'fri',\n",
       " 752: 'weeks',\n",
       " 753: 'sleeping',\n",
       " 754: 'ni8',\n",
       " 755: 'ha',\n",
       " 756: 'swing',\n",
       " 757: 'hee',\n",
       " 758: 'offers',\n",
       " 759: 'camcorder',\n",
       " 760: '08000930705',\n",
       " 761: 'invited',\n",
       " 762: 'wrong',\n",
       " 763: 'available',\n",
       " 764: 'savamob',\n",
       " 765: 'player',\n",
       " 766: 'mah',\n",
       " 767: 'finally',\n",
       " 768: 'type',\n",
       " 769: 'course',\n",
       " 770: 'park',\n",
       " 771: 'fone',\n",
       " 772: 'mates',\n",
       " 773: 'tired',\n",
       " 774: 'prob',\n",
       " 775: 'almost',\n",
       " 776: 'congratulations',\n",
       " 777: 'games',\n",
       " 778: 'code',\n",
       " 779: 'support',\n",
       " 780: 'least',\n",
       " 781: 'pics',\n",
       " 782: 'ans',\n",
       " 783: 'log',\n",
       " 784: \"wasn't\",\n",
       " 785: 'seen',\n",
       " 786: 'news',\n",
       " 787: 'area',\n",
       " 788: 'direct',\n",
       " 789: 'phones',\n",
       " 790: 'correct',\n",
       " 791: 'lovely',\n",
       " 792: 'visit',\n",
       " 793: 'surprise',\n",
       " 794: 'snow',\n",
       " 795: 'girls',\n",
       " 796: 'fancy',\n",
       " 797: 'crazy',\n",
       " 798: 'uk',\n",
       " 799: 'comin',\n",
       " 800: 'sea',\n",
       " 801: 'anytime',\n",
       " 802: 'paper',\n",
       " 803: 'bank',\n",
       " 804: 'friendship',\n",
       " 805: 'tscs',\n",
       " 806: 'mum',\n",
       " 807: 'eg',\n",
       " 808: 'pm',\n",
       " 809: 'loads',\n",
       " 810: 'case',\n",
       " 811: 'found',\n",
       " 812: 'nyt',\n",
       " 813: 'feels',\n",
       " 814: '200',\n",
       " 815: 'dating',\n",
       " 816: 'mom',\n",
       " 817: 'darlin',\n",
       " 818: 'operator',\n",
       " 819: 'quiz',\n",
       " 820: 'saturday',\n",
       " 821: 'w',\n",
       " 822: 'hand',\n",
       " 823: \"you'll\",\n",
       " 824: 'kiss',\n",
       " 825: 'hungry',\n",
       " 826: 'thinks',\n",
       " 827: 'valued',\n",
       " 828: 'laptop',\n",
       " 829: 'download',\n",
       " 830: 'motorola',\n",
       " 831: 'supposed',\n",
       " 832: 'march',\n",
       " 833: 'takes',\n",
       " 834: 'touch',\n",
       " 835: 'india',\n",
       " 836: 'ugh',\n",
       " 837: 'auction',\n",
       " 838: 'sort',\n",
       " 839: 'small',\n",
       " 840: 'asking',\n",
       " 841: 'booked',\n",
       " 842: 'reading',\n",
       " 843: 'myself',\n",
       " 844: 'father',\n",
       " 845: 'dnt',\n",
       " 846: 'information',\n",
       " 847: 'loving',\n",
       " 848: 'chikku',\n",
       " 849: 'their',\n",
       " 850: 'mobiles',\n",
       " 851: 'monday',\n",
       " 852: 'wine',\n",
       " 853: 'couple',\n",
       " 854: 'entered',\n",
       " 855: 'rock',\n",
       " 856: 'fantastic',\n",
       " 857: 'fucking',\n",
       " 858: 'cut',\n",
       " 859: 'difficult',\n",
       " 860: 'die',\n",
       " 861: 'seeing',\n",
       " 862: '2003',\n",
       " 863: 'pic',\n",
       " 864: 'wishing',\n",
       " 865: 'mayb',\n",
       " 866: 'reason',\n",
       " 867: '150p/msg',\n",
       " 868: 'lost',\n",
       " 869: 'truth',\n",
       " 870: 'move',\n",
       " 871: 'confirm',\n",
       " 872: 'party',\n",
       " 873: 'national',\n",
       " 874: 'ass',\n",
       " 875: \"isn't\",\n",
       " 876: 'christmas',\n",
       " 877: 'project',\n",
       " 878: 'usf',\n",
       " 879: 'crave',\n",
       " 880: '0800',\n",
       " 881: 'loved',\n",
       " 882: 'hit',\n",
       " 883: 'ex',\n",
       " 884: 'whenever',\n",
       " 885: 'missing',\n",
       " 886: 'age',\n",
       " 887: 'sim',\n",
       " 888: 'doin',\n",
       " 889: 'somewhere',\n",
       " 890: 'tickets',\n",
       " 891: 'sending',\n",
       " 892: 'fast',\n",
       " 893: 'muz',\n",
       " 894: 'staying',\n",
       " 895: 'plans',\n",
       " 896: 'save',\n",
       " 897: 'leaves',\n",
       " 898: 'blue',\n",
       " 899: 'mrng',\n",
       " 900: 'match',\n",
       " 901: 'mm',\n",
       " 902: 'talking',\n",
       " 903: 'reaching',\n",
       " 904: 'hospital',\n",
       " 905: 'gal',\n",
       " 906: 'checking',\n",
       " 907: 'whats',\n",
       " 908: 'kate',\n",
       " 909: 'via',\n",
       " 910: 'access',\n",
       " 911: 'reward',\n",
       " 912: '12',\n",
       " 913: 'yar',\n",
       " 914: 'knew',\n",
       " 915: 'ago',\n",
       " 916: 'cum',\n",
       " 917: 'ive',\n",
       " 918: 'forever',\n",
       " 919: 'lovable',\n",
       " 920: 'link',\n",
       " 921: 'gym',\n",
       " 922: 'starting',\n",
       " 923: 'sony',\n",
       " 924: 'company',\n",
       " 925: 'jay',\n",
       " 926: 'wana',\n",
       " 927: 'msgs',\n",
       " 928: 'moment',\n",
       " 929: 'near',\n",
       " 930: 'caller',\n",
       " 931: 'point',\n",
       " 932: 'store',\n",
       " 933: 'convey',\n",
       " 934: 'picking',\n",
       " 935: 'against',\n",
       " 936: 'st',\n",
       " 937: 'na',\n",
       " 938: 'ldn',\n",
       " 939: 'exam',\n",
       " 940: 'info',\n",
       " 941: 'rply',\n",
       " 942: 'unlimited',\n",
       " 943: 'happiness',\n",
       " 944: 'accept',\n",
       " 945: 'through',\n",
       " 946: 'eyes',\n",
       " 947: 'bslvyl',\n",
       " 948: 'noon',\n",
       " 949: 'england',\n",
       " 950: 'asap',\n",
       " 951: 'orchard',\n",
       " 952: 'empty',\n",
       " 953: '10p',\n",
       " 954: 'extra',\n",
       " 955: 'less',\n",
       " 956: 'pete',\n",
       " 957: '18+',\n",
       " 958: 'goodmorning',\n",
       " 959: 'ipod',\n",
       " 960: 'opt',\n",
       " 961: 'complimentary',\n",
       " 962: 'laugh',\n",
       " 963: 'write',\n",
       " 964: 'gotta',\n",
       " 965: 'spend',\n",
       " 966: 'discount',\n",
       " 967: '300',\n",
       " 968: 'otherwise',\n",
       " 969: 'questions',\n",
       " 970: 'bath',\n",
       " 971: 'slowly',\n",
       " 972: \"b'day\",\n",
       " 973: 'kiss*',\n",
       " 974: 'darren',\n",
       " 975: 'xx',\n",
       " 976: 'urself',\n",
       " 977: 'understand',\n",
       " 978: \"you'd\",\n",
       " 979: 'no1',\n",
       " 980: 'txting',\n",
       " 981: 'street',\n",
       " 982: 'months',\n",
       " 983: 'unless',\n",
       " 984: 'boss',\n",
       " 985: 'fixed',\n",
       " 986: 'telling',\n",
       " 987: 'valentines',\n",
       " 988: '\"',\n",
       " 989: 'gets',\n",
       " 990: 'cd',\n",
       " 991: 'studying',\n",
       " 992: 'met',\n",
       " 993: 'heard',\n",
       " 994: '350',\n",
       " 995: 'numbers',\n",
       " 996: 'rates',\n",
       " 997: 'hiya',\n",
       " 998: \"joy's\",\n",
       " 999: 'add',\n",
       " ...}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trenowanie modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- link: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "Zaimportuj model `RandomForestClassifier`. Użyj `.fit` na ztokenizowanym zbiorze treningowym. Użyj `predict` ztokenizowanym zbiorze treningowym oraz testowym. Zapisz wyniki w osobnych zmiennych.\n",
    "\n",
    "Nie zapomnij ustawić parametru `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "model.fit(X_train_tokenized, y_train)\n",
    "\n",
    "train_pred = model.predict(X_train_tokenized)\n",
    "test_pred = model.predict(X_test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprawdzanie jakości modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyj dowolnej metryki, alby sprawdzić jakość modelu. Wyświetl wynik dla danych treningowych i testowych. \n",
    "\n",
    "\n",
    "Zwróć uwagę na rozkład klas. Jest przewaga normalnych wiadomości (ham) nad wiadomościami typu spam. Oznacza to, że accuracy_score nie będzie najlepszą metryką do sprawdzania jakości modelu. Spróbuj poeksperymentować z metrykami takimi jak `f1_score`, `fbeta_score`, `precision`, `recall` a może `auc`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9921259842519685\n",
      "Test score: 0.8712871287128713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "train_score = f1_score(train_pred, y_train)\n",
    "print(\"Train score: {}\".format(train_score))\n",
    "\n",
    "test_score = f1_score(test_pred, y_test)\n",
    "print(\"Test score: {}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Słowa końcowe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem tego ćwiczenia było głównie pokazanie, w jaki sposób działa technika Bag of Words i próba zaimplementowania jej samodzielnie. \n",
    "\n",
    "Nie nastawiony model `RandomForestClassifier` zwraca wynik blisku 90% poprawności a jednocześnie odrobinę overfittuje. Jeżeli zdecydujesz się na przybliżenie obu wyników (train/test) to pamiętaj, że musisz dodatkowo wydzielić zbiór `validation`, na którym będziesz eksperymentować z parametrami modelu zanim ostatecznie zdecydujesz sie sprawdzić na zbiorze `test`.\n",
    "\n",
    "Ponieważ ilość klas jest bardzo niezbalansowana musisz upewnić się, że każda klasa została proporcjonalnie podzielona - jeżeli zbiór treningowy to 0.6, walidacyjny to 0.2 i treningowy to 0.2, to w każdym z tych zbiorów powinna się znaleść taka sama proporcja każdej klasy. \n",
    "\n",
    "Zalecane jest też stosowanie `cross_validation`. W takich sytuacjach funkcja `StratifiedKFold` jest bardzo pomocna:\n",
    "- link: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czasami zamiast nastawiać model, warto jest spróbować innego algorytmu. Przykładowo NaiveBayes bardzo dobrze sobie radzi z tym problemem:\n",
    "- link: http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9702797202797203\n",
      "Test score: 0.9192546583850932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tokenized, y_train)\n",
    "\n",
    "train_pred = model.predict(X_train_tokenized)\n",
    "test_pred = model.predict(X_test_tokenized)\n",
    "\n",
    "train_score = f1_score(train_pred, y_train)\n",
    "print(\"Train score: {}\".format(train_score))\n",
    "\n",
    "test_score = f1_score(test_pred, y_test)\n",
    "print(\"Test score: {}\".format(test_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
